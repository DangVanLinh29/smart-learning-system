import numpy as np
from sklearn.linear_model import LinearRegression
import random
import hashlib
import pandas as pd
import requests
import os
from dotenv import load_dotenv
import json
import sqlite3
from datetime import datetime, timedelta
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import google.generativeai as genai

# ƒê√£ n·∫°p file .env ƒë·ªÉ l·∫•y key
load_dotenv()
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")  # L·∫•y key Gemini

if not YOUTUBE_API_KEY:
    print("‚ö†Ô∏è C·∫¢NH B√ÅO: Ch∆∞a n·∫°p ƒë·ªß YOUTUBE_API_KEY")
if not GEMINI_API_KEY:
    print("‚ö†Ô∏è C·∫¢NH B√ÅO: Ch∆∞a n·∫°p ƒë·ªß GEMINI_API_KEY. AI tr√¨nh g·ª≠i s·∫Ω b·ªã T·∫Øt.")
else:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        print("‚úÖ ƒê√£ n·∫°p th√†nh c√¥ng Google Gemini AI.")
    except Exception as e:
        print(f"‚ùå L·ªói khi c·∫•u h√¨nh Gemini AI: {e}")
        GEMINI_API_KEY = None

# =========================================================
# Kh·ªüi t·∫°o h·ªá th·ªëng cache (AI + YouTube)
# =========================================================
DB_NAME = os.path.join(os.path.dirname(__file__), "ai_youtube_cache.db")

def init_cache_db():
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS ai_cache (
            prompt TEXT PRIMARY KEY,
            response TEXT,
            expires_at TEXT
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS youtube_cache (
            query TEXT PRIMARY KEY,
            result TEXT,
            expires_at TEXT
        )
    """)
    conn.commit()
    conn.close()
    print("‚úÖ Cache DB s·∫µn s√†ng (ai_cache + youtube_cache).")

init_cache_db()

# --- H√†m cache AI ---
def get_ai_cache(prompt):
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("SELECT response, expires_at FROM ai_cache WHERE prompt=?", (prompt,))
    row = c.fetchone()
    conn.close()
    if row:
        response, expires_at = row
        if datetime.now() < datetime.fromisoformat(expires_at):
            print(f"CACHE HIT: AI cache cho '{prompt[:40]}...'")
            return json.loads(response)
        else:
            print(f"CACHE EXPIRED: AI cache cho '{prompt[:40]}...'")
    return None

def set_ai_cache(prompt, response):
    expires_at = (datetime.now() + timedelta(hours=12)).isoformat()
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""
        INSERT OR REPLACE INTO ai_cache (prompt, response, expires_at)
        VALUES (?, ?, ?)
    """, (prompt, json.dumps(response), expires_at))
    conn.commit()
    conn.close()
    print(f"CACHE SET: AI cache cho '{prompt[:40]}...'")

# --- H√†m cache YouTube ---
def get_youtube_cache(query):
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("SELECT result, expires_at FROM youtube_cache WHERE query=?", (query,))
    row = c.fetchone()
    conn.close()
    if row:
        result, expires_at = row
        if datetime.now() < datetime.fromisoformat(expires_at):
            print(f"CACHE HIT: YouTube cache cho '{query[:40]}...'")
            return json.loads(result)
        else:
            print(f"CACHE EXPIRED: YouTube cache cho '{query[:40]}...'")
    return None

def set_youtube_cache(query, result):
    expires_at = (datetime.now() + timedelta(hours=12)).isoformat()
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""
        INSERT OR REPLACE INTO youtube_cache (query, result, expires_at)
        VALUES (?, ?, ?)
    """, (query, json.dumps(result), expires_at))
    conn.commit()
    conn.close()
    print(f"CACHE SET: YouTube cache cho '{query[:40]}...'")

# --- C√°c h√†m x·ª≠ l√Ω d·ªØ li·ªáu c∆° b·∫£n (KH√îNG ƒë·ªïi) ---

def process_tlu_data_to_progress(tlu_marks_data, student_id):
    progress_list = []
    found_real_data = False 
    if not isinstance(tlu_marks_data, list):
        return generate_mock_data(student_id)
    for subject in tlu_marks_data:
        if not isinstance(subject, dict):
            continue 
        try:
            subject_name = subject.get("subject", {}).get("subjectName", "N/A").title()
            score = subject.get("mark") 
            if (isinstance(score, (int, float))):
                progress = int(score * 10) 
                found_real_data = True
            else:
                continue 
            progress_list.append({"course": subject_name, "progress": max(0, min(100, progress))})
        except Exception as e:
            print(f"ERROR: Failed processing one subject (Mark). Reason: {e}")
    if not found_real_data:
        return generate_mock_data(student_id)
    return pd.DataFrame(progress_list)

def process_schedule_to_courses(schedule_data, student_id): 
    processed_subjects, processed_list = set(), []
    if not isinstance(schedule_data, list):
        return pd.DataFrame(processed_list)
    for subject in schedule_data:
        try:
            if subject is None:
                continue 
            subject_details = subject.get("courseSubject", {}).get("semesterSubject", {}).get("subject", {}) or {}
            subject_name = subject_details.get("subjectName", "N/A").title()
            subject_code = subject_details.get("subjectCode", "N/A")
            teacher_details = subject.get("courseSubject", {}).get("teacher", {}) or {}
            teacher_name = teacher_details.get("displayName", "N/A")
            if subject_name != "N/A" and subject_code != "N/A":
                if subject_code not in processed_subjects:
                    processed_subjects.add(subject_code)
                    processed_list.append({"course": subject_name, "subjectCode": subject_code, "teacherName": teacher_name, "progress": 0})
        except Exception as e:
            print(f"ERROR: L·ªói khi x·ª≠ l√Ω m·ªôt m√¥n h·ªçc (Schedule): {e}")
    if not processed_list:
        return generate_mock_data(student_id) 
    return pd.DataFrame(processed_list)

def generate_mock_data(student_id):
    mock_courses = [
        {"course": "L·∫≠p Tr√¨nh Game (Mock)", "base": 85}, 
        {"course": "Ph√°t Tri·ªÉn ƒê∆∞·ªùng D·ª´ng (Mock)", "base": 90},
        {"course": "C∆° S·ªü D·ªØ Li·ªáu (Mock)", "base": 70}, 
        {"course": "M·∫°ng M√°y T√≠nh (Mock)", "base": 65},
    ]
    progress_list = []
    for item in mock_courses:
        seed_val = int(hashlib.sha1(f"{student_id}{item['course']}".encode('utf-8')).hexdigest(), 16) % (10**8)
        random.seed(seed_val)
        progress = (item['base'] + random.randint(-15, 10))
        progress_list.append({"course": item['course'], "progress": max(40, min(100, progress))})
    return pd.DataFrame(progress_list)

# --- C√°c h√†m AI (logic & insight) ---

def build_cf_model_data(csv_data):
    try:
        data = csv_data[['M√£ SV', 'T√™n M√¥n H·ªçc', 'ƒêi·ªÉm T·ªïng K·∫øt (10)']].copy()
        data = data.dropna(subset=['ƒêi·ªÉm T·ªïng K·∫øt (10)'])
        data['T√™n M√¥n H·ªçc'] = data['T√™n M√¥n H·ªçc'].str.title()
        utility_matrix = data.pivot_table(index='M√£ SV', columns='T√™n M√¥n H·ªçc', values='ƒêi·ªÉm T·ªïng K·∫øt (10)').fillna(0)
        similarity_matrix = pd.DataFrame(cosine_similarity(utility_matrix.values),
                                         index=utility_matrix.index,
                                         columns=utility_matrix.index)
        return utility_matrix, similarity_matrix
    except Exception as e:
        print(f"L·ªói khi x√¢y d·ª±ng m√¥ h√¨nh CF: {e}")
        return None, None

def get_cf_recommendations(student_id_int, utility_matrix, similarity_matrix, num_recs=5):
    try:
        if student_id_int not in similarity_matrix.index:
            return []
        sim_scores = similarity_matrix[student_id_int].drop(student_id_int)
        top_neighbors = sim_scores.nlargest(5).index
        if top_neighbors.empty:
            return []
        neighbor_scores = utility_matrix.loc[top_neighbors]
        avg_scores = neighbor_scores.mean(axis=0)
        user_scores = utility_matrix.loc[student_id_int]
        unseen_courses = user_scores[user_scores == 0].index
        if unseen_courses.empty:
            return []
        recommended_scores = avg_scores[unseen_courses]
        top_cf_recs = recommended_scores.nlargest(num_recs)
        return [{"course": course, "predicted_score": round(score, 1)} 
                for course, score in top_cf_recs.items() if score > 0]
    except Exception as e:
        print(f"L·ªói khi t√≠nh to√°n CF: {e}")
        return []

# =========================================================
# H√†m Insight (K-Means, rule-based) -- phi√™n b·∫£n ƒë∆°n gi·∫£n
# =========================================================
def get_insight_logic(progress_data):
    """
    Ph√¢n t√≠ch AI d·ª±a tr√™n quy t·∫Øc r√µ r√†ng.
    """
    if progress_data.empty:
        return {"insights": ["Kh√¥ng ƒë·ªß d·ªØ li·ªáu ti·∫øn ƒë·ªô ƒë·ªÉ ph√¢n t√≠ch."]}
    progresses = progress_data["progress"].tolist()
    if not progresses:
        return {"insights": ["Kh√¥ng c√≥ d·ªØ li·ªáu ti·∫øn ƒë·ªô ƒë·ªÉ ph√¢n t√≠ch."]}
    insights = []
    # --- 1. Ph√¢n t√≠ch c∆° b·∫£n (Trung b√¨nh & ƒê·ªô l·ªách chu·∫©n) ---
    avg_prog = np.mean(progresses)
    std_dev = np.std(progresses)
    insights.append(f"ƒêi·ªÉm ti·∫øn ƒë·ªô trung b√¨nh c·ªßa b·∫°n l√† {avg_prog:.1f}%.")
    if std_dev > 20:
        insights.append(f"Hi·ªáu su·∫•t kh√¥ng ·ªïn ƒë·ªãnh (ƒë·ªô l·ªách chu·∫©n kho·∫£ng {std_dev:.1f}%), c·∫ßn ch·ªânh l·ªãch l√†m.")
    elif std_dev > 10:
        insights.append(f"B·∫°n h·ªçc kh√° ·ªïn (ƒë·ªô l·ªách chu·∫©n kho·∫£ng {std_dev:.1f}%), ti·∫øp t·ª•c ph√°t huy.")
    else:
        insights.append(f"B·∫°n h·ªçc r·∫•t ·ªïn ƒë·ªãnh (ƒë·ªô l·ªách chu·∫©n kho·∫£ng {std_dev:.1f}%), c√°c m√¥n c√≥ k·∫øt qu·∫£ t∆∞∆°ng ƒë·ªìng.")
    # --- 2. Ph√¢n t√≠ch theo quy t·∫Øc ---
    try:
        strong_courses = progress_data[progress_data['progress'] >= 80]['course'].tolist()
        weak_courses = progress_data[progress_data['progress'] < 60]['course'].tolist()
        if strong_courses:
            insights.append(f"Nh√≥m m√¥n TH√ÄNH C√îNG (ƒëi·ªÉm >= 80): {', '.join(strong_courses)}.")
        if weak_courses:
            insights.append(f"Nh√≥m m√¥n C·∫¶N C·∫¢I THI·ªÜN (ƒëi·ªÉm < 60): {', '.join(weak_courses)}.")
        if not strong_courses and not weak_courses:
            insights.append("T·∫•t c·∫£ c√°c m√¥n ƒë·ªÅu ƒëang ·ªü m·ª©c trung b√¨nh (60-80%).")
    except Exception as e:
        print(f"L·ªói khi ch·∫°y ph√¢n t√≠ch insight theo quy t·∫Øc: {e}")
    return {"insights": insights}

def predict_future_logic(progress_data):
    future_preds = []
    if progress_data.empty:
        return {"predictions": []}
    for index, row in progress_data.iterrows():
        current_progress = float(row["progress"])
        past_scores = np.clip(np.random.normal(current_progress, 5, size=5), 40, 100)
        X = np.arange(1, 6).reshape(-1, 1)
        model = LinearRegression().fit(X, past_scores)
        next_week = float(model.predict([[6]])[0])
        risk = max(0, min(100, 100 - next_week))
        future_preds.append({
            "course": row["course"],
            "predicted_progress": round(next_week, 1),
            "risk": round(risk, 1)
        })
    warnings = [
        {
            "course": r["course"],
            "predicted_progress": r["predicted_progress"],
            "risk": r["risk"],
            "advice": ("‚ö†Ô∏è C·∫ßn c·ªë g·∫Øng!" if r["predicted_progress"] < 60 else "‚úÖ ƒê√£ t·ªët!")
        }
        for r in sorted(future_preds, key=lambda x: -x["risk"])
    ]
    return {"predictions": warnings}

# =========================================================
# H√†m g·ªçi GEMINI AI ƒë·ªÉ g·ª£i √Ω h·ªçc t·∫≠p
# =========================================================
def generate_ai_driven_content(course_name, progress):
    if not GEMINI_API_KEY:
        print("T·∫Øt AI: Kh√¥ng c√≥ GEMINI_API_KEY.")
        return None
    prompt = f"AI_GEMINI_{course_name}_{progress}"
    cached = get_ai_cache(prompt)
    if cached:
        return cached
    try:
        model = genai.GenerativeModel("gemini-2.0-flash")
        prompt_text = f"""
        M·ªôt sinh vi√™n Vi·ªát Nam ƒëang h·ªçc y·∫øu m√¥n "{course_name}" (ti·∫øn ƒë·ªô: {progress}%).
        T·∫°o JSON c√≥ d·∫°ng:
        {{
          "roadmap": ["L·ªùi khuy√™n 1", "L·ªùi khuy√™n 2", "L·ªùi khuy√™n 3", "L·ªùi khuy√™n 4"],
          "video_topics": ["ch·ªß ƒë·ªÅ video 1", "ch·ªß ƒë·ªÅ video 2", "ch·ªß ƒë·ªÅ video 3"]
        }}
        """
        print(f"‚û°Ô∏è ƒêang g·ªçi Gemini AI cho m√¥n: {course_name}...")
        response = model.generate_content(
            prompt_text,
            generation_config=genai.types.GenerationConfig(response_mime_type="application/json")
        )
        cleaned_text = response.text.strip().replace("```json", "").replace("```", "")
        ai_content = json.loads(cleaned_text)
        set_ai_cache(prompt, ai_content)
        print(f"‚úÖ AI tr·∫£ v·ªÅ g·ª£i √Ω cho: {course_name}")
        return ai_content
    except Exception as e:
        print(f"‚ùå L·ªói khi g·ªçi Gemini AI cho m√¥n {course_name}: {e}")
        return None

def get_fallback_recommendation(course_name, progress):
    """
    H√†m d·ª± ph√≤ng (fallback) n·∫øu AI b·ªã l·ªói.
    D√πng template c·ªë ƒë·ªãnh.
    """
    print(f"‚ö†Ô∏è D√πng g·ª£i √Ω d·ª± ph√≤ng (template) cho m√¥n: {course_name}")
    roadmap = [
        f"X√°c ƒë·ªãnh l·∫°i kh·ªëi n·ªÅn t·∫£ng c·ªßa m√¥n {course_name} (hi·ªán t·∫°i {progress}%)."
    ]
    if progress < 50:
        roadmap.append("B·∫Øt ƒë·∫ßu l·∫°i v·ªõi c√°c b√†i gi·∫£ng c∆° b·∫£n, t·∫≠p trung v√†o n·ªÅn t·∫£ng.")
    else:
        roadmap.append("T·∫≠p trung v√†o c√°c ch·ªß ƒë·ªÅ n√¢ng cao v√† b√†i t·∫≠p l·ªõn c·∫ßn v∆∞·ª£t tr·ªôi.")
    roadmap.append("T√¨m ƒë∆∞·ªùng h∆∞·ªõng h·ªçc v·ªõi gi·∫£ng vi√™n ho·∫∑c ng∆∞·ªùi c√≥ kinh nghi·ªám.")
    videos = search_youtube_videos(f"b√†i gi·∫£ng {course_name}")
    query_safe_course = course_name.replace(' ', '+')
    documents = [
        {
            "title": f"T·∫£i t√†i li·ªáu {course_name}",
            "url": f"https://www.google.com/search?q=t·∫£i+{query_safe_course}+pdf"
        }
    ]
    exercises = [
        {
            "title": f"T√¨m b√†i t·∫≠p {course_name}",
            "url": f"https://www.google.com/search?q=b√†i+t·∫≠p+{query_safe_course}"
        }
    ]
    return {
        "roadmap": roadmap,
        "videos": videos,
        "documents": documents,
        "exercises": exercises
    }

# =========================================================
# H√†m logic ch√≠nh g·ª£i √Ω h·ªçc t·∫≠p
# =========================================================
def get_recommendation_logic(progress_data, student_id_int, cf_model_data, materials_db=None):
    """
    Logic g·ª£i √Ω t·ªïng h·ª£p:
    1. G·ª£i √Ω 'C·∫ßn c·∫£i thi·ªán': D·ª±a tr√™n Gemini AI
    2. G·ª£i √Ω 'Kh√°m ph√°': D·ª±a tr√™n m√¥ h√¨nh CF (CSV)
    """
    # --- 1. G·ª£i √Ω 'C·∫ßn c·∫£i thi·ªán' (TLU API + Gemini AI) ---
    improve_recommendations = []
    low_courses = [row for index, row in progress_data.iterrows() if row["progress"] < 70]
    for course_data in low_courses:
        course = course_data["course"]
        progress = course_data["progress"]
        ai_content = generate_ai_driven_content(course, progress)
        roadmap, videos, documents, exercises = [], [], [], []
        if ai_content:
            if isinstance(ai_content, dict):
                roadmap = ai_content.get("roadmap", [])
                video_topics = ai_content.get("video_topics", [])
            elif isinstance(ai_content, list):
                if len(ai_content) > 0 and isinstance(ai_content[0], dict):
                    roadmap = ai_content[0].get("roadmap", [])
                    video_topics = ai_content[0].get("video_topics", [])
                else:
                    roadmap = ai_content
                    video_topics = []
            else:
                roadmap, video_topics = [], []

            for topic in video_topics:
                videos.extend(search_youtube_videos(topic, max_results=1))
            query_safe_course = course.replace(' ', '+')
            documents = [
                {"title": f"T·∫£i t√†i li·ªáu {course} (Google)", "url": f"https://www.google.com/search?q=t·∫£i+{query_safe_course}+pdf"}
            ]
            exercises = [
                {"title": f"T√¨m b√†i t·∫≠p {course} (Google)", "url": f"https://www.google.com/search?q=b√†i+t·∫≠p+{query_safe_course}"}
            ]
        else:
            fallback_data = get_fallback_recommendation(course, progress)
            roadmap = fallback_data["roadmap"]
            videos = fallback_data["videos"]
            documents = fallback_data["documents"]
            exercises = fallback_data["exercises"]

        improve_recommendations.append({
            "course": course,
            "progress": progress,
            "roadmap": roadmap,
            "resources": {"videos": videos, "documents": documents, "exercises": exercises}
        })
    # --- 2. G·ª£i √Ω 'Kh√°m ph√°' (m√¥ h√¨nh CF) ---
    discover_recommendations = []
    if cf_model_data and student_id_int:
        utility_matrix, similarity_matrix = cf_model_data
        if utility_matrix is not None and similarity_matrix is not None:
            discover_recommendations = get_cf_recommendations(student_id_int,
                                                               utility_matrix,
                                                               similarity_matrix,
                                                               num_recs=5)
    # --- 3. T·ªïng h·ª£p k·∫øt qu·∫£ ---
    message = "D∆∞·ªõi ƒë√¢y l√† c√°c g·ª£i √Ω t·ªët nh·∫•t cho b·∫°n."
    if not improve_recommendations and not discover_recommendations:
        message = "üéâ B·∫°n h·ªçc t·ªët! AI kh√¥ng t√¨m th·∫•y g·ª£i √Ω n√†o c·∫ßn thi·∫øt."
    elif not improve_recommendations:
        message = "üîç C√°c m√¥n h·ªçc c·ªßa b·∫°n kh√° ·ªïn! D∆∞·ªõi ƒë√¢y l√† g·ª£i √Ω kh√°m ph√° th√™m."
    return {
        "message": message,
        "improve_recommendations": improve_recommendations,
        "discover_recommendations": discover_recommendations
    }

import requests, urllib.parse

def search_youtube_videos(query, max_results=2):
    if not YOUTUBE_API_KEY:
        print("‚ùå Thi·∫øu API key YouTube.")
        return []
    academic_keywords = " h·ªçc t·∫≠p OR b√†i gi·∫£ng OR course OR university OR tutorial OR gi·ªõi thi·ªáu h·ªçc OR cybersecurity"
    full_query = f"{query} {academic_keywords}"
    encoded_query = urllib.parse.quote(full_query)
    url = (
        f"https://www.googleapis.com/youtube/v3/search?"
        f"part=snippet&type=video&maxResults={max_results}"
        f"&regionCode=VN&relevanceLanguage=vi"
        f"&safeSearch=strict&order=relevance"
        f"&q={encoded_query}&key={YOUTUBE_API_KEY}"
    )
    try:
        res = requests.get(url)
        res.raise_for_status()
        data = res.json()
        videos = []
        for item in data.get("items", []):
            vid = item["id"]["videoId"]
            title = item["snippet"]["title"]
            if not any(word in title.lower() for word in ["kickfit", "boxing", "nh·∫£y", "review", "vlog"]):
                videos.append({
                    "title": title,
                    "url": f"https://www.youtube.com/watch?v={vid}",
                })
        return videos
    except Exception as e:
        print(f"‚ùå L·ªói YouTube: {e}")
        return []
